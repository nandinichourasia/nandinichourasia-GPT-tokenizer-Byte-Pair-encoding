# Byte-Pair Encoding (BPE)
It is the method used by large language models like GPT-3.5 and GPT-4 to break text into smaller units(tokens).
BPE is a sub-word based tokenization algorithm.The word Based tokenization Algorithm was unable to handle the words which are out of the vocabulary , but here in Byte Pair encoding Tokens are created using characters and sub words.

# âœ… Features & Advantages of BPE over Word-Based Tokenization
ğŸ”¤ Handles unknown words<br>
ğŸ’¬ Robust to new words	<br>
ğŸ“‰ Reduces vocabulary size<br>	
ğŸ“š Learns meaningful word parts<br>
âœ‚ï¸ Token reuse across words<br>
ğŸ’¾ Efficient memory usage<br>

