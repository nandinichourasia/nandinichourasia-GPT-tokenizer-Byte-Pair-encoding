# Byte-Pair Encoding (BPE)
It is the method used by large language models like GPT-3.5 and GPT-4 to break text into smaller units(tokens).
BPE is a sub-word based tokenization algorithm.The word Based tokenization Algorithm was unable to handle the words which are out of the vocabulary , but here in Byte Pair encoding Tokens are created using characters and sub words.

# âœ… Features & Advantages of BPE over Word-Based Tokenization
ðŸ”¤ Handles unknown words<br>
ðŸ’¬ Robust to new words	<br>
ðŸ“‰ Reduces vocabulary size<br>	
ðŸ“š Learns meaningful word parts<br>

